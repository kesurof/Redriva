import os
import sys
import argparse
import asyncio
import aiohttp
import sqlite3
import time
import signal
import logging
import json
from pathlib import Path

def load_env_file():
    """Charge les variables d'environnement depuis le fichier .env"""
    env_file = Path(__file__).parent.parent / '.env'
    if env_file.exists():
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    # Ne pas √©craser les variables d√©j√† d√©finies
                    if key.strip() not in os.environ:
                        os.environ[key.strip()] = value.strip()

# Chargement des variables d'environnement depuis .env
load_env_file()

# Chargement des variables d'environnement depuis .env
load_env_file()

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Interruption propre
stop_requested = False
def handle_sigint(signum, frame):
    global stop_requested
    logging.warning("Interruption clavier re√ßue (CTRL+C), arr√™t propre...")
    stop_requested = True

signal.signal(signal.SIGINT, handle_sigint)

# Configuration via variables d'environnement avec valeurs par d√©faut
RD_API_URL = "https://api.real-debrid.com/rest/1.0/torrents"
DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '../data/redriva.db'))
MAX_CONCURRENT = int(os.getenv('RD_MAX_CONCURRENT', '50'))
BATCH_SIZE = int(os.getenv('RD_BATCH_SIZE', '250'))
QUOTA_WAIT_TIME = int(os.getenv('RD_QUOTA_WAIT', '60'))
TORRENT_WAIT_TIME = int(os.getenv('RD_TORRENT_WAIT', '10'))

def load_token():
    token = os.environ.get("RD_TOKEN")
    if not token:
        config_path = os.path.join(os.path.dirname(__file__), "../config/rd_token.conf")
        if os.path.exists(config_path):
            with open(config_path) as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        token = line
                        break
    if not token:
        logging.error("Token Real-Debrid introuvable.")
        sys.exit(1)
    return token

def init_db():
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute('''CREATE TABLE IF NOT EXISTS torrents (
            id TEXT PRIMARY KEY,
            filename TEXT,
            status TEXT,
            bytes INTEGER,
            added_on TEXT
        )''')
        c.execute('''CREATE TABLE IF NOT EXISTS torrent_details (
            id TEXT PRIMARY KEY,
            name TEXT,
            status TEXT,
            size INTEGER,
            files_count INTEGER,
            added TEXT,
            downloaded INTEGER,
            speed INTEGER,
            progress REAL,
            hash TEXT,
            original_filename TEXT,
            host TEXT,
            split INTEGER,
            links TEXT,
            ended TEXT,
            error TEXT,
            links_count INTEGER,
            folder TEXT,
            priority TEXT,
            custom1 TEXT
        )''')
        conn.commit()

async def api_request(session, url, headers, params=None, max_retries=3):
    """Fonction g√©n√©rique pour les appels API avec gestion d'erreurs"""
    for attempt in range(max_retries):
        if stop_requested:
            return None
        try:
            async with session.get(url, headers=headers, params=params) as resp:
                if resp.status == 401 or resp.status == 403:
                    logging.error("Token Real-Debrid invalide ou expir√©.")
                    sys.exit(1)
                if resp.status == 404:
                    return None
                if resp.status == 429:
                    wait_time = QUOTA_WAIT_TIME if params else TORRENT_WAIT_TIME
                    logging.warning(f"Quota API d√©pass√©, attente {wait_time}s...")
                    await asyncio.sleep(wait_time)
                    continue
                resp.raise_for_status()
                return await resp.json()
        except Exception as e:
            if attempt == max_retries - 1:
                logging.error(f"Erreur API apr√®s {max_retries} tentatives: {e}")
                return None
            await asyncio.sleep(2 ** attempt)  # Backoff exponentiel
    return None

def upsert_torrent_detail(detail):
    if not detail or not detail.get('id'):
        return
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute('''INSERT OR REPLACE INTO torrent_details
            (id, name, status, size, files_count, added, downloaded, speed, progress, hash, original_filename, host, split, links, ended, error, links_count, folder, priority, custom1)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
            (
                detail.get('id'),
                detail.get('filename') or detail.get('name'),
                detail.get('status'),
                detail.get('bytes'),
                len(detail.get('files', [])),
                detail.get('added'),
                detail.get('downloaded'),
                detail.get('speed'),
                detail.get('progress'),
                detail.get('hash'),
                detail.get('original_filename'),
                detail.get('host'),
                detail.get('split'),
                ",".join(detail.get('links', [])) if detail.get('links') else None,
                detail.get('ended'),
                detail.get('error'),
                detail.get('links_count'),
                detail.get('folder'),
                detail.get('priority'),
                None
            )
        )
        conn.commit()

def upsert_torrent(t):
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute('''INSERT OR REPLACE INTO torrents (id, filename, status, bytes, added_on)
            VALUES (?, ?, ?, ?, ?)''',
            (t.get('id'), t.get('filename'), t.get('status'), t.get('bytes'), t.get('added'),)
        )
        conn.commit()

async def fetch_all_torrents(token):
    headers = {"Authorization": f"Bearer {token}"}
    limit = 1000
    page = 1
    total = 0
    
    async with aiohttp.ClientSession() as session:
        while True:
            if stop_requested:
                logging.info("Arr√™t demand√©, interruption de la r√©cup√©ration des torrents.")
                break
            params = {"page": page, "limit": limit}
            torrents = await api_request(session, RD_API_URL, headers, params)
            
            if not torrents:
                break
                
            for t in torrents:
                upsert_torrent(t)
            total += len(torrents)
            logging.info(f"Page {page}: {len(torrents)} torrents (total: {total})")
            page += 1
    return total

async def fetch_torrent_detail(session, token, torrent_id):
    url = f"https://api.real-debrid.com/rest/1.0/torrents/info/{torrent_id}"
    headers = {"Authorization": f"Bearer {token}"}
    
    detail = await api_request(session, url, headers)
    if detail:
        logging.info(f"D√©tail r√©cup√©r√© pour {torrent_id}")
        upsert_torrent_detail(detail)
    return detail

async def fetch_all_torrent_details(token, torrent_ids, max_concurrent=MAX_CONCURRENT):
    semaphore = asyncio.Semaphore(max_concurrent)
    total_processed = 0
    
    async with aiohttp.ClientSession() as session:
        async def process_torrent(tid):
            async with semaphore:
                return await fetch_torrent_detail(session, token, tid)
        
        # Traitement par batch pour respecter les quotas
        for i in range(0, len(torrent_ids), BATCH_SIZE):
            if stop_requested:
                logging.info("Arr√™t demand√©, interruption de la r√©cup√©ration des d√©tails.")
                break
                
            batch_ids = torrent_ids[i:i+BATCH_SIZE]
            tasks = [process_torrent(tid) for tid in batch_ids]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Compter les succ√®s
            successful = sum(1 for r in batch_results if r and not isinstance(r, Exception))
            total_processed += successful
            
            logging.info(f"Batch {i//BATCH_SIZE + 1}: {successful}/{len(batch_ids)} d√©tails r√©cup√©r√©s")
            
            # Pause entre les batches sauf pour le dernier
            if i + BATCH_SIZE < len(torrent_ids):
                logging.info(f"Pause {QUOTA_WAIT_TIME}s avant le prochain batch...")
                await asyncio.sleep(QUOTA_WAIT_TIME)
    
    return total_processed

def show_stats():
    """Affiche les statistiques de la base de donn√©es"""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("SELECT COUNT(*) FROM torrents")
        total_torrents = c.fetchone()[0]
        
        c.execute("SELECT COUNT(*) FROM torrent_details")
        total_details = c.fetchone()[0]
        
        c.execute("SELECT status, COUNT(*) FROM torrents GROUP BY status")
        status_counts = dict(c.fetchall())
        
        print(f"\nüìä Statistiques Redriva:")
        print(f"   Torrents: {total_torrents}")
        print(f"   D√©tails: {total_details}")
        print(f"   Status: {status_counts}")

def sync_details_only(token, status_filter=None):
    """Synchronise uniquement les d√©tails des torrents existants"""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        query = "SELECT id FROM torrents"
        params = ()
        if status_filter:
            query += " WHERE status = ?"
            params = (status_filter,)
        c.execute(query, params)
        torrent_ids = [row[0] for row in c.fetchall()]
    
    if not torrent_ids:
        logging.info("Aucun torrent trouv√© pour synchronisation des d√©tails.")
        return
        
    logging.info(f"Synchronisation des d√©tails pour {len(torrent_ids)} torrents...")
    processed = asyncio.run(fetch_all_torrent_details(token, torrent_ids))
    logging.info(f"D√©tails synchronis√©s pour {processed} torrents.")

def sync_all(token):
    """Synchronisation compl√®te"""
    total = asyncio.run(fetch_all_torrents(token))
    logging.info(f"Synchronisation termin√©e. {total} torrents enregistr√©s.")
    
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("SELECT id FROM torrents")
        torrent_ids = [row[0] for row in c.fetchall()]
    
    logging.info(f"R√©cup√©ration des d√©tails de {len(torrent_ids)} torrents...")
    processed = asyncio.run(fetch_all_torrent_details(token, torrent_ids))
    logging.info(f"D√©tails r√©cup√©r√©s pour {processed} torrents.")

def clear_database():
    """Vide compl√®tement la base de donn√©es"""
    try:
        with sqlite3.connect(DB_PATH) as conn:
            c = conn.cursor()
            c.execute("DELETE FROM torrent_details")
            details_deleted = c.rowcount
            c.execute("DELETE FROM torrents")
            torrents_deleted = c.rowcount
            conn.commit()
            
        print(f"‚úÖ Base de donn√©es vid√©e avec succ√®s:")
        print(f"   - {torrents_deleted} torrents supprim√©s")
        print(f"   - {details_deleted} d√©tails supprim√©s")
        
    except Exception as e:
        logging.error(f"Erreur lors du vidage de la base : {e}")

# === NOUVELLES FONCTIONS OPTIMIS√âES ===

class DynamicRateLimiter:
    def __init__(self, initial_concurrent=20, max_concurrent=80):
        self.concurrent = initial_concurrent
        self.max_concurrent = max_concurrent
        self.success_count = 0
        self.error_count = 0
        self.last_adjustment = time.time()
        
    def adjust_concurrency(self, success=True):
        if success:
            self.success_count += 1
        else:
            self.error_count += 1
            
        if (self.success_count + self.error_count) % 50 == 0 or time.time() - self.last_adjustment > 30:
            error_rate = self.error_count / max(1, self.success_count + self.error_count)
            
            if error_rate < 0.05:
                self.concurrent = min(self.max_concurrent, int(self.concurrent * 1.2))
                logging.info(f"üìà Concurrence augment√©e √† {self.concurrent}")
            elif error_rate > 0.15:
                self.concurrent = max(5, int(self.concurrent * 0.7))
                logging.info(f"üìâ Concurrence r√©duite √† {self.concurrent}")
                
            self.last_adjustment = time.time()
            self.success_count = self.error_count = 0
            
    def get_semaphore(self):
        return asyncio.Semaphore(self.concurrent)

def save_progress(processed_ids, filename="data/sync_progress.json"):
    """Sauvegarde la progression"""
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    with open(filename, 'w') as f:
        json.dump({
            'processed_ids': list(processed_ids),
            'timestamp': time.time()
        }, f)

def load_progress(filename="data/sync_progress.json"):
    """Charge la progression pr√©c√©dente"""
    try:
        with open(filename, 'r') as f:
            data = json.load(f)
            if time.time() - data['timestamp'] < 21600:  # 6 heures
                return set(data['processed_ids'])
    except:
        pass
    return set()

def get_torrents_needing_update():
    """Torrents n√©cessitant une mise √† jour (version basique)"""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute('''
            SELECT t.id FROM torrents t
            LEFT JOIN torrent_details td ON t.id = td.id
            WHERE td.id IS NULL 
               OR (td.status IN ('downloading', 'queued', 'waiting_files_selection'))
               OR datetime('now') - datetime(t.added_on) > 7
        ''')
        return [row[0] for row in c.fetchall()]

async def detect_changes_smart(token):
    """D√©tecte les changements pr√©cis n√©cessitant une mise √† jour"""
    changes = {
        'new_torrents': [],
        'status_changes': [],
        'missing_details': [],
        'active_downloads': [],
        'error_retry': []
    }
    
    # R√©cup√©rer la liste basique des torrents RD
    logging.info("üîç Analyse des changements en cours...")
    rd_torrents = await fetch_torrents_list(token)
    
    # Obtenir les torrents locaux
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute('''
            SELECT t.id, t.status as t_status, t.filename, 
                   td.status as td_status, td.progress, td.error
            FROM torrents t
            LEFT JOIN torrent_details td ON t.id = td.id
        ''')
        local_torrents = {row[0]: {
            'id': row[0],
            'status': row[1],
            'filename': row[2],
            'detail_status': row[3],
            'progress': row[4],
            'error': row[5],
            'has_details': row[3] is not None
        } for row in c.fetchall()}
    
    rd_ids = set()
    
    # Analyser chaque torrent RD
    for rd_torrent in rd_torrents:
        rd_id = rd_torrent['id']
        rd_ids.add(rd_id)
        rd_status = rd_torrent.get('status', '')
        
        local = local_torrents.get(rd_id)
        
        if not local:
            # Nouveau torrent
            changes['new_torrents'].append(rd_id)
        elif not local['has_details']:
            # D√©tails manquants
            changes['missing_details'].append(rd_id)
        elif local['detail_status'] != rd_status and rd_status:
            # Changement de statut d√©tect√©
            changes['status_changes'].append(rd_id)
        elif rd_status in ['downloading', 'queued', 'waiting_files_selection']:
            # T√©l√©chargement actif
            changes['active_downloads'].append(rd_id)
        elif local['error'] and rd_status not in ['error', 'virus', 'dead']:
            # Retry des erreurs si le statut RD n'est plus en erreur
            changes['error_retry'].append(rd_id)
    
    # V√©rifier les torrents supprim√©s de RD
    local_ids = set(local_torrents.keys())
    deleted_ids = local_ids - rd_ids
    if deleted_ids:
        logging.info(f"üóëÔ∏è  {len(deleted_ids)} torrents supprim√©s de Real-Debrid d√©tect√©s")
    
    return changes

async def fetch_torrents_list(token):
    """R√©cup√®re la liste basique des torrents depuis Real-Debrid"""
    headers = {"Authorization": f"Bearer {token}"}
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(RD_API_URL, headers=headers) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    logging.error(f"Erreur API: {response.status}")
                    return []
        except Exception as e:
            logging.error(f"Erreur lors de la r√©cup√©ration des torrents: {e}")
            return []

async def fetch_all_torrent_details_v2(token, torrent_ids, resumable=False):
    """Version optimis√©e de fetch_all_torrent_details"""
    if resumable:
        processed_ids = load_progress()
        remaining_ids = [tid for tid in torrent_ids if tid not in processed_ids]
        if processed_ids:
            logging.info(f"üìÇ Reprise: {len(processed_ids)} d√©j√† trait√©s, {len(remaining_ids)} restants")
    else:
        remaining_ids = torrent_ids
        processed_ids = set()
    
    if not remaining_ids:
        logging.info("‚úÖ Tous les d√©tails sont √† jour !")
        return len(processed_ids)
    
    rate_limiter = DynamicRateLimiter()
    total_processed = len(processed_ids)
    start_time = time.time()
    
    # Pool de connexions optimis√©
    connector = aiohttp.TCPConnector(limit=100, limit_per_host=50, keepalive_timeout=30)
    timeout = aiohttp.ClientTimeout(total=15, connect=5)
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        
        async def process_torrent_optimized(tid):
            semaphore = rate_limiter.get_semaphore()
            async with semaphore:
                result = await fetch_torrent_detail(session, token, tid)
                success = result is not None
                rate_limiter.adjust_concurrency(success)
                
                if success:
                    nonlocal total_processed
                    total_processed += 1
                    processed_ids.add(tid)
                    
                    # Stats temps r√©el + sauvegarde
                    if total_processed % 100 == 0:
                        elapsed = time.time() - start_time
                        rate = (total_processed - len(processed_ids)) / elapsed if elapsed > 0 else 0
                        remaining = len(torrent_ids) - total_processed
                        eta = remaining / rate if rate > 0 else 0
                        
                        logging.info(f"üìä {total_processed}/{len(torrent_ids)} | "
                                   f"{rate:.1f}/s | ETA: {eta/60:.1f}min | "
                                   f"Concurrence: {rate_limiter.concurrent}")
                        
                        if resumable:
                            save_progress(processed_ids)
                
                return result
        
        # Traitement par chunks adaptatifs
        chunk_size = min(300, len(remaining_ids))
        for i in range(0, len(remaining_ids), chunk_size):
            if stop_requested:
                break
                
            chunk_ids = remaining_ids[i:i+chunk_size]
            tasks = [process_torrent_optimized(tid) for tid in chunk_ids]
            
            await asyncio.gather(*tasks, return_exceptions=True)
            
            # Pause adaptative
            if i + chunk_size < len(remaining_ids):
                pause = max(3, 20 - rate_limiter.concurrent * 0.2)
                logging.info(f"‚è∏Ô∏è  Pause {pause:.1f}s...")
                await asyncio.sleep(pause)
    
    elapsed = time.time() - start_time
    processed_new = total_processed - len(set(processed_ids) - processed_ids)
    logging.info(f"üéâ Termin√© ! {processed_new} nouveaux d√©tails en {elapsed/60:.1f}min "
                 f"({processed_new/elapsed:.1f} torrents/s)")
    
    # Nettoyer le fichier de progression
    if resumable and os.path.exists("data/sync_progress.json"):
        os.remove("data/sync_progress.json")
    
    return total_processed

def sync_all_v2(token):
    """Version optimis√©e de sync_all"""
    logging.info("üöÄ Synchronisation rapide d√©marr√©e...")
    
    # √âtape 1: R√©cup√©rer tous les torrents (inchang√©)
    total = asyncio.run(fetch_all_torrents(token))
    logging.info(f"‚úÖ {total} torrents synchronis√©s")
    
    # √âtape 2: R√©cup√©ration optimis√©e des d√©tails
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("SELECT id FROM torrents")
        torrent_ids = [row[0] for row in c.fetchall()]
    
    logging.info(f"üîÑ R√©cup√©ration optimis√©e des d√©tails pour {len(torrent_ids)} torrents...")
    processed = asyncio.run(fetch_all_torrent_details_v2(token, torrent_ids, resumable=True))
    logging.info(f"üéØ Synchronisation rapide termin√©e ! {processed} d√©tails trait√©s")

def sync_smart(token, args=None):
    """Synchronisation intelligente (seulement ce qui a chang√©)"""
    logging.info("üß† Synchronisation intelligente d√©marr√©e...")
    
    # D√©tecter les changements avec la nouvelle fonction
    changes = asyncio.run(detect_changes_smart(token))
    
    # Appliquer les filtres selon les options
    if args:
        if args.skip_error_retry:
            changes['error_retry'] = []
            
        if not args.include_active_downloads:
            changes['active_downloads'] = []
            
        if args.status_changes_only:
            # Garder seulement les changements de statut et nouveaux torrents
            changes = {
                'new_torrents': changes['new_torrents'],
                'status_changes': changes['status_changes'],
                'missing_details': [],
                'active_downloads': [],
                'error_retry': []
            }
    
    # Calculer le total des changements
    total_changes = sum(len(changes[key]) for key in changes)
    
    if total_changes == 0:
        logging.info("‚úÖ Rien √† synchroniser, tout est √† jour !")
        return
    
    # Mode dry-run : afficher ce qui serait fait sans le faire
    if args and args.dry_run:
        logging.info("üîç Mode dry-run : aucune modification ne sera effectu√©e")
        logging.info("   Les torrents suivants seraient mis √† jour :")
        
        priority_order = [
            ('new_torrents', 'üÜï'),
            ('status_changes', 'üîÑ'),
            ('active_downloads', '‚¨áÔ∏è'),
            ('error_retry', 'üîÑ'),
            ('missing_details', 'üìù')
        ]
        
        for key, emoji in priority_order:
            if changes[key]:
                logging.info(f"   {emoji} {key.replace('_', ' ').title()} : {len(changes[key])}")
                if args.verbose:
                    for tid in changes[key][:5]:  # Montrer les 5 premiers
                        logging.info(f"      ‚Ä¢ {tid}")
                    if len(changes[key]) > 5:
                        logging.info(f"      ‚Ä¢ ... et {len(changes[key]) - 5} autres")
        return
    
    # Afficher le r√©sum√© des changements d√©tect√©s
    logging.info("üìä Changements d√©tect√©s :")
    if changes['new_torrents']:
        logging.info(f"   ÔøΩ Nouveaux torrents : {len(changes['new_torrents'])}")
    if changes['status_changes']:
        logging.info(f"   üîÑ Changements de statut : {len(changes['status_changes'])}")
    if changes['active_downloads']:
        logging.info(f"   ‚¨áÔ∏è  T√©l√©chargements actifs : {len(changes['active_downloads'])}")
    if changes['missing_details']:
        logging.info(f"   üìù D√©tails manquants : {len(changes['missing_details'])}")
    if changes['error_retry']:
        logging.info(f"   üîÑ Retry d'erreurs : {len(changes['error_retry'])}")
    
    logging.info(f"üéØ Total : {total_changes} torrents √† mettre √† jour")
    
    # Rassembler tous les IDs √† traiter par ordre de priorit√©
    priority_order = [
        'new_torrents',        # Nouveaux = priorit√© max
        'status_changes',      # Changements de statut
        'active_downloads',    # Downloads en cours
        'error_retry',         # Retry des erreurs
        'missing_details'      # D√©tails manquants
    ]
    
    torrent_ids = []
    for priority in priority_order:
        torrent_ids.extend(changes[priority])
    
    # √âviter les doublons tout en gardant l'ordre de priorit√©
    seen = set()
    unique_ids = []
    for tid in torrent_ids:
        if tid not in seen:
            seen.add(tid)
            unique_ids.append(tid)
    
    # Traiter les mises √† jour
    start_time = time.time()
    processed = asyncio.run(fetch_all_torrent_details_v2(token, unique_ids))
    end_time = time.time()
    
    # Statistiques finales
    duration = end_time - start_time
    rate = processed / duration if duration > 0 else 0
    
    logging.info(f"‚úÖ Synchronisation intelligente termin√©e !")
    logging.info(f"   üìä {processed} d√©tails mis √† jour en {duration:.1f}s ({rate:.1f}/s)")
    
    # Afficher un r√©sum√© des r√©sultats
    display_sync_summary()

def display_sync_summary():
    """Affiche un r√©sum√© des donn√©es synchronis√©es"""
    try:
        with sqlite3.connect(DB_PATH) as conn:
            c = conn.cursor()
            
            # Statistiques g√©n√©rales
            c.execute("SELECT COUNT(*) FROM torrents")
            total_torrents = c.fetchone()[0]
            
            c.execute("SELECT COUNT(*) FROM torrent_details")
            total_details = c.fetchone()[0]
            
            # R√©partition par statut
            c.execute("""
                SELECT COALESCE(td.status, t.status) as status, COUNT(*) as count
                FROM torrents t
                LEFT JOIN torrent_details td ON t.id = td.id
                GROUP BY COALESCE(td.status, t.status)
                ORDER BY count DESC
            """)
            status_counts = c.fetchall()
            
            # Torrents r√©cents (derni√®res 24h)
            c.execute("""
                SELECT COUNT(*) FROM torrents 
                WHERE datetime('now') - datetime(added_on) < 1
            """)
            recent_count = c.fetchone()[0]
            
            print(f"\nüìä R√©sum√© de la synchronisation :")
            print(f"   üìÇ Total torrents : {total_torrents}")
            print(f"   üìù Avec d√©tails : {total_details}")
            print(f"   üÜï R√©cents (24h) : {recent_count}")
            
            if status_counts:
                print(f"\nüìà R√©partition par statut :")
                for status, count in status_counts[:5]:  # Top 5
                    print(f"   {status or 'inconnu'} : {count}")
                    
    except Exception as e:
        logging.warning(f"Impossible d'afficher le r√©sum√© : {e}")

def sync_resume(token):
    """Reprendre une synchronisation interrompue"""
    logging.info("‚èÆÔ∏è  Reprise de synchronisation...")
    
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("SELECT id FROM torrents")
        all_ids = [row[0] for row in c.fetchall()]
    
    processed = asyncio.run(fetch_all_torrent_details_v2(token, all_ids, resumable=True))
    logging.info(f"‚úÖ Reprise termin√©e ! {processed} d√©tails trait√©s")

def sync_torrents_only(token):
    """Synchronisation uniquement des torrents de base (sans d√©tails)"""
    logging.info("üìã Synchronisation des torrents de base uniquement...")
    
    total = asyncio.run(fetch_all_torrents(token))
    
    if total > 0:
        logging.info(f"‚úÖ Synchronisation termin√©e ! {total} torrents enregistr√©s dans la table 'torrents'")
        
        # Afficher un petit r√©sum√©
        with sqlite3.connect(DB_PATH) as conn:
            c = conn.cursor()
            c.execute("SELECT status, COUNT(*) FROM torrents GROUP BY status")
            status_counts = dict(c.fetchall())
            
            print(f"\nüìä R√©sum√© des torrents synchronis√©s:")
            for status, count in status_counts.items():
                print(f"   {status}: {count}")
    else:
        logging.info("‚ÑπÔ∏è  Aucun torrent trouv√© ou synchronis√©")

def main():
    parser = argparse.ArgumentParser(description="Redriva - Sync Real-Debrid vers SQLite")
    parser.add_argument('--sync-all', action='store_true', help="Synchroniser tous les torrents")
    parser.add_argument('--details-only', action='store_true', help="Synchroniser uniquement les d√©tails des torrents existants")
    parser.add_argument('--status', help="Filtrer par status (downloaded, error, etc.)")
    parser.add_argument('--stats', action='store_true', help="Afficher les statistiques de la base")
    parser.add_argument('--clear', action='store_true', help="Vider compl√®tement la base de donn√©es")
    
    # === NOUVELLES OPTIONS OPTIMIS√âES ===
    parser.add_argument('--sync-fast', action='store_true', help="üöÄ Synchronisation rapide (version optimis√©e)")
    parser.add_argument('--sync-smart', action='store_true', help="üß† Synchronisation intelligente (seulement les changements)")
    parser.add_argument('--resume', action='store_true', help="‚èÆÔ∏è  Reprendre une synchronisation interrompue")
    parser.add_argument('--torrents-only', action='store_true', help="üìã Synchroniser uniquement les torrents de base (sans d√©tails)")
    
    # === OPTIONS POUR SYNC-SMART ===
    parser.add_argument('--include-active-downloads', action='store_true', help="Inclure les t√©l√©chargements actifs dans sync-smart")
    parser.add_argument('--skip-error-retry', action='store_true', help="Ignorer les retry d'erreurs dans sync-smart")
    parser.add_argument('--status-changes-only', action='store_true', help="Sync-smart : seulement les changements de statut")
    parser.add_argument('--dry-run', action='store_true', help="Afficher ce qui serait synchronis√© sans le faire")
    parser.add_argument('--verbose', action='store_true', help="Affichage d√©taill√© des op√©rations")
    
    args = parser.parse_args()

    token = load_token()
    init_db()

    try:
        if args.clear:
            # Demander confirmation avant de vider
            response = input("‚ö†Ô∏è  √ätes-vous s√ªr de vouloir vider la base de donn√©es ? (oui/non): ")
            if response.lower() in ['oui', 'o', 'yes', 'y']:
                clear_database()
            else:
                print("‚ùå Op√©ration annul√©e.")
        elif args.stats:
            show_stats()
        elif args.torrents_only:
            sync_torrents_only(token)   # NOUVELLE OPTION
        elif args.sync_fast:
            sync_all_v2(token)  # NOUVELLE VERSION RAPIDE
        elif args.sync_smart:
            sync_smart(token, args)   # NOUVELLE VERSION INTELLIGENTE
        elif args.resume:
            sync_resume(token)  # NOUVELLE VERSION RESUMABLE
        elif args.details_only:
            sync_details_only(token, args.status)  # ANCIENNE VERSION
        elif args.sync_all:
            sync_all(token)     # ANCIENNE VERSION
        else:
            parser.print_help()
    except KeyboardInterrupt:
        logging.warning("Arr√™t manuel par l'utilisateur.")
    except Exception as e:
        logging.error(f"Erreur inattendue : {e}")

if __name__ == "__main__":
    main()